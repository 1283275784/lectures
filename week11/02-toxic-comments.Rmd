---
title: "Classifying Personal Attacks"
author: "Blake Miller"
date: "27 March 2023"
output: html_document
---

## Classifying Personal Attacks

*Content warning: This problem makes use of data from a project to automate moderation of toxic speech online. Many comments in this dataset contain hate speech and upsetting content. Please take care as you work on this assignment.*

This exercise makes use of replication data for the paper [Ex Machina: Personal Attacks Seen at Scale](https://arxiv.org/abs/1610.08914) by Ellery Wulczyn, Nithum Thain, and Lucas Dixon. The paper introduces a method for crowd-sourcing labels for personal attacks and then draws several inferences about how personal attacks manifest on Wikipedia Talk Pages. They find that, "the majority of personal attacks on Wikipedia are not the result of a few malicious users, nor primarily the consequence of allowing anonymous contributions from unregistered users." We will use their data and SVM models to identify personal attacks.

Let's start by loading some required packages

```{r, warning=FALSE, message=FALSE}
library(doMC)
library(glmnet)
library(quanteda)
```

## Representing Text Features

### Preprocessing text with quanteda

Before we can do any type of automated text analysis, we will need to go through several "pre-processing" steps before it can be passed to a statistical model. We'll use the `quanteda` package  [quanteda](https://github.com/kbenoit/quanteda) here.

The basic unit of work for the `quanteda` package is called a `corpus`, which represents a collection of text documents with some associated metadata. Documents are the subunits of a corpus. You can use `summary` to get some information about your corpus.

```{r}
library(quanteda)
library(quanteda.textplots)

texts_train <- read.csv('train.csv', stringsAsFactors=F)
texts_test <- read.csv('test.csv', stringsAsFactors=F)

n_train <- nrow(texts_train)

corpus <- corpus(rbind(texts_train, texts_test), text_field="text") # create a corpus
corpus
```

We can then create a tokens object from the corpus using the `tokens` function. This gives us our terms which we will process to create features for our document feature matrix. `tokens` has many useful options (check out `?tokens` for more information).

```{r}
?tokens
toks <- tokens(corpus, remove_punct = TRUE, remove_url=TRUE, verbose=TRUE)
toks
```

Next we can create a document-feature matrix by passing our tokens into the `dfm` function. The `dfm` will show the count of times each word appears in each document (comment). To stem our documents we use the `SnowballC` package's implementation of the Porter stemmer. In a large corpus like this, many features often only appear in one or two documents. In some case it's a good idea to remove those features, to speed up the analysis or because they're not relevant. We can `trim` the dfm:

```{r}
toks_stop <- tokens_remove(toks, stopwords("english"))
toks_stem <- tokens_wordstem(toks_stop)
dfm_stem <- dfm(toks_stem, tolower=TRUE)
```

## Basic Text Classification

Let's train a logistic regression (family="binomial") with a LASSO penalty. We choose the optimal value of lambda using cross-validation with `cv.glmnet`. Using `plot`, we can plot error (binomial deviance) for all values of $\lambda$ chosen by `cv.glmnet`. How many non-zero coefficients are in the model where misclassification error is minimized? How many non-zero coefficients are in the model one standard deviation from where misclassification error is minimized?

```{r}
registerDoMC(cores=5) # trains all 5 folds in parallel (at once rather than one by one)
mod <- cv.glmnet(dfm_stem[1:n_train,], docvars(dfm_stem,"attack")[1:n_train], nfolds=5, parallel=TRUE, family="binomial")

plot(mod)
```

According to cross-validation error calculated by `cv.glm`, we can examine the optimal $\lambda$ stored in the output? We can then find the corresponding CV error for this value of $\lambda$.

```{r}
mod$lambda.min
log(mod$lambda.min) # To match the axis in the plot above

lam_min <- which(mod$lambda == mod$lambda.min)
lam_min
cv_min <- mod$cvm[lam_min]
cv_min
```

## Error Measures

We can evaluate test set performance for the best-fit model using accuracy.

```{r}
pred_min <- predict(mod, dfm_stem[-c(1:n_train),], s="lambda.min", type="class")
mean(pred_min == dfm_stem$attack[-c(1:n_train)])

lam_1se <- which(mod$lambda == mod$lambda.1se)
pred_1se <- predict(mod, dfm_stem[-c(1:n_train),], s="lambda.1se", type="class")
mean(pred_1se == dfm_stem$attack[-c(1:n_train)])
```

We can also examine the confusion matrix to get a better idea of the error. We can also use this confusion matrix to calculate other error measures using the functions specified below.

```{r}
table(pred_min, dfm_stem$attack[-c(1:n_train)])
table(pred_1se, dfm_stem$attack[-c(1:n_train)])

## function to compute accuracy
accuracy <- function(ypred, y){
	tab <- table(ypred, y)
	return(sum(diag(tab))/sum(tab))
}
# function to compute precision
precision <- function(ypred, y){
	tab <- table(ypred, y)
	return((tab[2,2])/(tab[2,1]+tab[2,2]))
}
# function to compute recall
recall <- function(ypred, y){
	tab <- table(ypred, y)
	return(tab[2,2]/(tab[1,2]+tab[2,2]))
}

cat("Accuracy: ", accuracy(pred_min, dfm_stem$attack[-c(1:n_train)]))
cat("Precision: ", precision(pred_min, dfm_stem$attack[-c(1:n_train)]))
cat("Recall: ", recall(pred_min, dfm_stem$attack[-c(1:n_train)]))
```

Let's dig a bit deeper into the performance of the model. How would it predict some test cases where context and word dependencies are important? First, let's create a function that takes in a new comment and outputs what the model's prediction is.


```{r}
library(knitr)

new_texts <- c(
  "You might think you are great, but you're not that brilliant.",
  "You'd probably have more success in your edits if you tried growing a brain first.",
  "You're not very helpful. These edits are not as fabulous as you might think.",
  "While sounding smart and interesting to a lay audience, you're quite uninformed and are hopelessly ignorant."
)

corpus <- corpus(new_texts)
toks <- tokens(corpus, remove_punct = TRUE, remove_url=TRUE, verbose=TRUE)
toks_stop <- tokens_remove(toks, stopwords("english"))
toks_stem <- tokens_wordstem(toks_stop)
new_dfm_stem <- dfm(toks_stem, tolower=TRUE)
new_dfm_stem

# Match the vocabulary of the new dfm with the original dfm
new_dfm_matched <- dfm_match(new_dfm_stem, featnames(dfm_stem))

pred_1se <- predict(mod, new_dfm_matched, s="lambda.1se", type="class")

kable(cbind(new_texts, pred_1se), col.names = c("text", "attack"))
```